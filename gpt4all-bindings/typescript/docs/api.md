<!-- Generated by documentation.js. Update this documentation by updating the source code. -->

### Table of Contents

*   [download][1]
    *   [Parameters][2]
    *   [Examples][3]
*   [DownloadOptions][4]
    *   [location][5]
    *   [debug][6]
    *   [url][7]
*   [DownloadController][8]
    *   [cancel][9]
    *   [promise][10]
*   [ModelType][11]
*   [ModelFile][12]
    *   [gptj][13]
    *   [llama][14]
    *   [mpt][15]
*   [type][16]
*   [LLModel][17]
    *   [constructor][18]
        *   [Parameters][19]
    *   [type][20]
    *   [name][21]
    *   [stateSize][22]
    *   [threadCount][23]
    *   [setThreadCount][24]
        *   [Parameters][25]
    *   [raw\_prompt][26]
        *   [Parameters][27]
    *   [isModelLoaded][28]
    *   [setLibraryPath][29]
        *   [Parameters][30]
    *   [getLibraryPath][31]
*   [createCompletion][32]
    *   [Parameters][33]
    *   [Examples][34]
*   [CompletionOptions][35]
    *   [verbose][36]
    *   [hasDefaultHeader][37]
    *   [hasDefaultFooter][38]
*   [PromptMessage][39]
    *   [role][40]
    *   [content][41]
*   [prompt\_tokens][42]
*   [completion\_tokens][43]
*   [total\_tokens][44]
*   [CompletionReturn][45]
    *   [model][46]
    *   [usage][47]
    *   [choices][48]
*   [CompletionChoice][49]
    *   [message][50]
*   [LLModelPromptContext][51]
    *   [logits\_size][52]
    *   [tokens\_size][53]
    *   [n\_past][54]
    *   [n\_ctx][55]
    *   [n\_predict][56]
    *   [top\_k][57]
    *   [top\_p][58]
    *   [temp][59]
    *   [n\_batch][60]
    *   [repeat\_penalty][61]
    *   [repeat\_last\_n][62]
    *   [context\_erase][63]
*   [createTokenStream][64]
    *   [Parameters][65]
*   [DEFAULT\_DIRECTORY][66]
*   [DEFAULT\_LIBRARIES\_DIRECTORY][67]

## download

Initiates the download of a model file of a specific model type.
By default this downloads without waiting. use the controller returned to alter this behavior.

### Parameters

*   `model` **[ModelFile][12]** The model file to be downloaded.
*   `options` **[DownloadOptions][4]** to pass into the downloader. Default is { location: (cwd), debug: false }.

### Examples

```javascript
const controller = download('ggml-gpt4all-j-v1.3-groovy.bin')
controller.promise().then(() => console.log('Downloaded!'))
```

*   Throws **[Error][68]** If the model already exists in the specified location.
*   Throws **[Error][68]** If the model cannot be found at the specified url.

Returns **[DownloadController][8]** object that allows controlling the download process.

## DownloadOptions

Options for the model download process.

### location

location to download the model.
Default is process.cwd(), or the current working directory

Type: [string][69]

### debug

Debug mode -- check how long it took to download in seconds

Type: [boolean][70]

### url

Remote download url. Defaults to `https://gpt4all.io/models`

Type: [string][69]

## DownloadController

Model download controller.

### cancel

Cancel the request to download from gpt4all website if this is called.

Type: function (): void

### promise

Convert the downloader into a promise, allowing people to await and manage its lifetime

Type: function (): [Promise][71]\<void>

## ModelType

Type of the model

Type: (`"gptj"` | `"llama"` | `"mpt"`)

## ModelFile

Full list of models available

### gptj

List of GPT-J Models

Type: (`"ggml-gpt4all-j-v1.3-groovy.bin"` | `"ggml-gpt4all-j-v1.2-jazzy.bin"` | `"ggml-gpt4all-j-v1.1-breezy.bin"` | `"ggml-gpt4all-j.bin"`)

### llama

List Llama Models

Type: (`"ggml-gpt4all-l13b-snoozy.bin"` | `"ggml-vicuna-7b-1.1-q4_2.bin"` | `"ggml-vicuna-13b-1.1-q4_2.bin"` | `"ggml-wizardLM-7B.q4_2.bin"` | `"ggml-stable-vicuna-13B.q4_2.bin"` | `"ggml-nous-gpt4-vicuna-13b.bin"`)

### mpt

List of MPT Models

Type: (`"ggml-mpt-7b-base.bin"` | `"ggml-mpt-7b-chat.bin"` | `"ggml-mpt-7b-instruct.bin"`)

## type

Model architecture. This argument currently does not have any functionality and is just used as descriptive identifier for user.

Type: [ModelType][11]

## LLModel

LLModel class representing a language model.
This is a base class that provides common functionality for different types of language models.

### constructor

Initialize a new LLModel.

#### Parameters

*   `path` **[string][69]** Absolute path to the model file.

<!---->

*   Throws **[Error][68]** If the model file does not exist.

### type

either 'gpt', mpt', or 'llama' or undefined

Returns **([ModelType][11] | [undefined][72])**&#x20;

### name

The name of the model.

Returns **[ModelFile][12]**&#x20;

### stateSize

Get the size of the internal state of the model.
NOTE: This state data is specific to the type of model you have created.

Returns **[number][73]** the size in bytes of the internal state of the model

### threadCount

Get the number of threads used for model inference.
The default is the number of physical cores your computer has.

Returns **[number][73]** The number of threads used for model inference.

### setThreadCount

Set the number of threads used for model inference.

#### Parameters

*   `newNumber` **[number][73]** The new number of threads.

Returns **void**&#x20;

### raw\_prompt

Prompt the model with a given input and optional parameters.
This is the raw output from std out.
Use the prompt function exported for a value

#### Parameters

*   `q` **[string][69]** The prompt input.
*   `params` **Partial<[LLModelPromptContext][51]>?** Optional parameters for the prompt context.

Returns **any** The result of the model prompt.

### isModelLoaded

Whether the model is loaded or not.

Returns **[boolean][70]**&#x20;

### setLibraryPath

Where to search for the pluggable backend libraries

#### Parameters

*   `s` **[string][69]**&#x20;

Returns **void**&#x20;

### getLibraryPath

Where to get the pluggable backend libraries

Returns **[string][69]**&#x20;

## createCompletion

The nodejs equivalent to python binding's chat\_completion

### Parameters

*   `llmodel` **[LLModel][17]** The language model object.
*   `messages` **[Array][74]<[PromptMessage][39]>** The array of messages for the conversation.
*   `options` **[CompletionOptions][35]** The options for creating the completion.

### Examples

```javascript
const llmodel = new LLModel(model)
const messages = [ 
{ role: 'system', message: 'You are a weather forecaster.' },
{ role: 'user', message: 'should i go out today?' } ]
const completion = await createCompletion(llmodel, messages, {
 verbose: true,
 temp: 0.9,
})
console.log(completion.choices[0].message.content)
// No, it's going to be cold and rainy.
```

Returns **[CompletionReturn][45]** The completion result.

## CompletionOptions

**Extends Partial\<LLModelPromptContext>**

The options for creating the completion.

### verbose

Indicates if verbose logging is enabled.

Type: [boolean][70]

### hasDefaultHeader

Indicates if the default header is included in the prompt.

Type: [boolean][70]

### hasDefaultFooter

Indicates if the default footer is included in the prompt.

Type: [boolean][70]

## PromptMessage

A message in the conversation, identical to OpenAI's chat message.

### role

The role of the message.

Type: (`"system"` | `"assistant"` | `"user"`)

### content

The message content.

Type: [string][69]

## prompt\_tokens

The number of tokens used in the prompt.

Type: [number][73]

## completion\_tokens

The number of tokens used in the completion.

Type: [number][73]

## total\_tokens

The total number of tokens used.

Type: [number][73]

## CompletionReturn

The result of the completion, similar to OpenAI's format.

### model

The model name.

Type: [ModelFile][12]

### usage

Token usage report.

Type: {prompt\_tokens: [number][73], completion\_tokens: [number][73], total\_tokens: [number][73]}

### choices

The generated completions.

Type: [Array][74]<[CompletionChoice][49]>

## CompletionChoice

A completion choice, similar to OpenAI's format.

### message

Response message

Type: [PromptMessage][39]

## LLModelPromptContext

Model inference arguments for generating completions.

### logits\_size

The size of the raw logits vector.

Type: [number][73]

### tokens\_size

The size of the raw tokens vector.

Type: [number][73]

### n\_past

The number of tokens in the past conversation.

Type: [number][73]

### n\_ctx

The number of tokens possible in the context window.

Type: [number][73]

### n\_predict

The number of tokens to predict.

Type: [number][73]

### top\_k

The top-k logits to sample from.

Type: [number][73]

### top\_p

The nucleus sampling probability threshold.

Type: [number][73]

### temp

The temperature to adjust the model's output distribution.

Type: [number][73]

### n\_batch

The number of predictions to generate in parallel.

Type: [number][73]

### repeat\_penalty

The penalty factor for repeated tokens.

Type: [number][73]

### repeat\_last\_n

The number of last tokens to penalize.

Type: [number][73]

### context\_erase

The percentage of context to erase if the context window is exceeded.

Type: [number][73]

## createTokenStream

TODO: Help wanted to implement this

### Parameters

*   `llmodel` **[LLModel][17]**&#x20;
*   `messages` **[Array][74]<[PromptMessage][39]>**&#x20;
*   `options` **[CompletionOptions][35]**&#x20;

Returns **function (ll: [LLModel][17]): AsyncGenerator<[string][69]>**&#x20;

## DEFAULT\_DIRECTORY

From python api:
models will be stored in (homedir)/.cache/gpt4all/\`

Type: [string][69]

## DEFAULT\_LIBRARIES\_DIRECTORY

From python api:
The default path for dynamic libraries to be stored.
You may separate paths by a semicolon to search in multiple areas.
This searches DEFAULT\_DIRECTORY/libraries, cwd/libraries, and finally cwd.

Type: [string][69]

[1]: #download

[2]: #parameters

[3]: #examples

[4]: #downloadoptions

[5]: #location

[6]: #debug

[7]: #url

[8]: #downloadcontroller

[9]: #cancel

[10]: #promise

[11]: #modeltype

[12]: #modelfile

[13]: #gptj

[14]: #llama

[15]: #mpt

[16]: #type

[17]: #llmodel

[18]: #constructor

[19]: #parameters-1

[20]: #type-1

[21]: #name

[22]: #statesize

[23]: #threadcount

[24]: #setthreadcount

[25]: #parameters-2

[26]: #raw_prompt

[27]: #parameters-3

[28]: #ismodelloaded

[29]: #setlibrarypath

[30]: #parameters-4

[31]: #getlibrarypath

[32]: #createcompletion

[33]: #parameters-5

[34]: #examples-1

[35]: #completionoptions

[36]: #verbose

[37]: #hasdefaultheader

[38]: #hasdefaultfooter

[39]: #promptmessage

[40]: #role

[41]: #content

[42]: #prompt_tokens

[43]: #completion_tokens

[44]: #total_tokens

[45]: #completionreturn

[46]: #model

[47]: #usage

[48]: #choices

[49]: #completionchoice

[50]: #message

[51]: #llmodelpromptcontext

[52]: #logits_size

[53]: #tokens_size

[54]: #n_past

[55]: #n_ctx

[56]: #n_predict

[57]: #top_k

[58]: #top_p

[59]: #temp

[60]: #n_batch

[61]: #repeat_penalty

[62]: #repeat_last_n

[63]: #context_erase

[64]: #createtokenstream

[65]: #parameters-6

[66]: #default_directory

[67]: #default_libraries_directory

[68]: https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Error

[69]: https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String

[70]: https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Boolean

[71]: https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Promise

[72]: https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/undefined

[73]: https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Number

[74]: https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Array
